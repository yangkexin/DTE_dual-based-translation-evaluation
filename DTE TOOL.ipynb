{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Get retranslation sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules import *\n",
    "from TrainingUtils import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"data/transformer_cn.bin\"\n",
    "vocab_path = \"data/vocab_cn.pkl\"\n",
    "spcial_dict_path = \"data/dict6000.pkl\"\n",
    "input_data_path = \"\"\n",
    "data_save_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_segment(str1,vocab):\n",
    "    str2 = []\n",
    "    i=0\n",
    "    while i< len(str1):\n",
    "        if str1[i] in vocab.keys():\n",
    "            refer_vocab = vocab[str1[i]]\n",
    "            view_window = len(max(refer_vocab, key=len)) + i#找出包含该字词典\n",
    "            if view_window >len(str1):\n",
    "                view_window = len(str1)\n",
    "            use_flag = False\n",
    "            while view_window >= i+1:\n",
    "                if str1[i:view_window] in refer_vocab:\n",
    "                    str2.append(\"\".join(str1[i:view_window]))\n",
    "                    use_flag= True\n",
    "                    break\n",
    "                else:view_window -= 1\n",
    "            if use_flag == True:\n",
    "                i = view_window\n",
    "            else:\n",
    "                str2.append(str1[i])\n",
    "                i = i + 1\n",
    "        else:\n",
    "            str2.append(str1[i])\n",
    "            i = i +1\n",
    "    return str2\n",
    "\n",
    "def load_model(path,vocab_path):\n",
    "    params = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "    state_dict = params['state_dict']\n",
    "\n",
    "    SRC,TGT = pickle.load(open(vocab_path, \"rb\"))\n",
    "\n",
    "\n",
    "    model = make_model(SRC,TGT, N=6,d_model=512, d_ff=2048, h=8,dropout=0.1)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model = model.cuda()\n",
    "    model = model.eval()\n",
    "    return model,SRC,TGT\n",
    "\n",
    "def interactive(line,model,SRC,TGT):\n",
    "    vocab = pickle.load(open(special_dict_path, \"rb\"))\n",
    "    line = \"\".join(line.split())\n",
    "    line = vocab_segment(line,vocab)\n",
    "    line = [word for word in \" \".join(line)]\n",
    "    line = [SRC.stoi[w] for w in line]\n",
    "    src_len = len(line)\n",
    "    src_sent = torch.LongTensor([line]).cuda()\n",
    "    src_mask = torch.ones(1, 1, src_len).cuda()\n",
    "    return line_greedy_decode(100,model, src_sent, src_mask,TGT.stoi[\"<s>\"],TGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,SRC,TGT = load_model(model_path, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = datareading(input_data_path)\n",
    "output = []\n",
    "for line in input_data:\n",
    "    output.append(interactive(line,model,SRC,TGT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefiletxt(output,data_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. File input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "def eachFile(filepath):\n",
    "    child = []\n",
    "    filename = []\n",
    "    pathDir = os.listdir(filepath)\n",
    "    pathDir = sorted(pathDir)\n",
    "    for allDir in pathDir:\n",
    "        child.append(\n",
    "            os.path.join('%s%s' % (filepath, allDir)))\n",
    "        filename.append(allDir)\n",
    "    return filename,child\n",
    "\n",
    "def datareading(filepath):\n",
    "    text = []\n",
    "    file = open(filepath, \"r\",encoding=\"utf-8\")\n",
    "    for line in file.readlines():\n",
    "        text.append(\"\".join(line.strip().split()))\n",
    "        \n",
    "    text = [[word for word in line] for line in text]\n",
    "    text = [\" \".join(line) for line in text]\n",
    "    return text\n",
    "\n",
    "def savefiletxt(text,filepath):\n",
    "    f = open(filepath,\"w\",encoding='utf-8')\n",
    "    for i in text:\n",
    "        f.write(str(i))\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu = datareading(\"AMCT/trans27.1_an.txt\") # Model retranslation output\n",
    "bai = datareading(\"AMCT/trans27.1_cn.txt\") # Model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu_refer = datareading(\"AMCT/trans_27.1_an_refer.txt\") # Ancient Chinese reference\n",
    "bai_refer = datareading(\"AMCT/trans_27.1_cn_refer.txt\") # Modern Chinese reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dual BLEU computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import *\n",
    "from bleu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_bleu(refers,outputs):\n",
    "    bleu_scores = []\n",
    "    for i in range(len(refers)) :\n",
    "        refer = refers[i]\n",
    "        output = outputs[i]\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(\n",
    "            [[refer]],[output],max_order=2,smooth=True)\n",
    "        bleu_scores.append(bleu)\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_bleu_score = sentences_bleu(gu_refer,gu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Average BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentences_bleu(refers,outputs):\n",
    "    bleu_scores = []\n",
    "    for i in range(len(refers)) :\n",
    "        refer = refers[i]\n",
    "        bleu_score = 0.\n",
    "        for j in range(len(outputs)):\n",
    "            output = outputs[j]\n",
    "            bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(\n",
    "                [[refer]],[output],max_order=2,smooth=True)\n",
    "            bleu_score += bleu\n",
    "        bleu_scores.append(bleu_score/len(refers))\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_average = average_sentences_bleu(gu_refer,gu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Sentence  similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vectors_a, vectors_b):\n",
    "    sims = []\n",
    "    for i in range(len(vectors_a)):\n",
    "        vector_a = np.mat(vectors_a[i])\n",
    "        vector_b = np.mat(vectors_b[i])\n",
    "        num = float(vector_a * vector_b.T)\n",
    "        denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "        cos = num / denom\n",
    "        sim = 0.5 + 0.5 * cos\n",
    "        sims.append(sim)\n",
    "    return sims\n",
    "\n",
    "def average_embed(text,vocab_dict):\n",
    "    text = [line.split()for line in text]    \n",
    "    sentence_embedding = []\n",
    "    for i in range(len(text)):\n",
    "        line_embeding = np.full((512), 0, dtype=\"float32\")\n",
    "        for j in range(len(text[i])):\n",
    "            line_embeding += vocab_dict[text[i][j]][0]\n",
    "        sentence_embedding.append(list(line_embeding/[len(text[i])]))\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict=pickle.load(open(\"order/emb_word2vec_an.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu_refer_w2c = average_embed(gu_refer,embed_dict)\n",
    "gu_w2c =  average_embed(gu,embed_dict)\n",
    "sent_sim = cos_sim(gu_w2c, gu_refer_w2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. DTE computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_normal(score):\n",
    "    score_mean = np.mean(score)  \n",
    "    score_std = np.std(score)\n",
    "    score_normal = []\n",
    "    for i in range(len(score)):\n",
    "        score_normal.append((float(score[i]) - score_mean)/score_std)\n",
    "    return score_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_normal = score_normal(dual_bleu_score)\n",
    "avg_bleu_normal = score_normal(average_sentences_bleu)\n",
    "sent_sim_normal = score_normal(sent_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTE_sent_score = dual_bleu_normal + avg_bleu_normal + sent_sim_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTE_corpus_score = np.mean(DTE_sent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
